,,"Voting (XGBoost, CatBoost, LGBM)",,,Public Score
Version,Notes,Mean Train QWK,Mean Validation QWK,Optimized QWK,
V10,"Combined static data with time-series data on the id column
For categorical columns (columns containing 'season'), missing data was filled with 'Missing', and categorical values were mapped to numerical values (e.g., 'Missing' → 0, 'Spring' → 1).
Feature selection: 58 static features that appear in both the test and train datasets, plus 96 features from the time-series data
In the train dataset, the 'sii' column is the target variable, so rows with missing values in the 'sii' column were removed

Resulting dataset dimensions:
Train: 2736 rows × 155 columns
Test: 20 rows × 154 columns

Model trained and predictions made using Stratified K-Fold cross-validation",0.9583,0.3757,,
V10.1,"Applied out-of-fold predictions to ensure no data leakage during validation
Stored test predictions in each fold, then averaged them to reduce variance and improve generalization in the final model prediction
Applied threshold optimization to fine-tune prediction boundaries and improve QWK (Quadratic Weighted Kappa) score",0.8820,0.3863,0.452,
V11-14,Fixed issues encountered while submitting to Kaggle,0.8820,0.3863,0.452,0.455
V15-16,"Fine-tuned hyperparameters of the XGBoost model:

max_depth: [3, 5, 7, 10] → 3
learning_rate: [0.01, 0.05, 0.1, 0.2] → 0.05
n_estimators: [100, 200, 300] → 100",0.8245,0.3940,0.463,0.454
V17,"Fine-tuned hyperparameters of the XGBoost model:

max_depth: [3]
learning_rate: [0.05]
n_estimators: [100]
subsample: [0.6, 0.8, 1.0] → 0.6
colsample_bytree: [0.6, 0.8, 1.0] → 1.0
reg_lambda: [0.1, 1, 10] → 1
reg_alpha: [0, 1, 5] → 5",0.8155,0.3947,0.470,0.465
V18,"Fine-tuned hyperparameters of the XGBoost model using a random grid search with 100 iterations:

max_depth: [3, 5, 7, 10]
learning_rate: [0.01, 0.05, 0.1, 0.2]
n_estimators: [100, 200, 300]
subsample: [0.6, 0.8, 1.0]
colsample_bytree: [0.6, 0.8, 1.0]
reg_lambda: [0.1, 1, 10]
reg_alpha: [0, 1, 5]

Best Parameters:

subsample: 0.6
reg_lambda: 10
reg_alpha: 5
n_estimators: 300
max_depth: 7
learning_rate: 0.01
colsample_bytree: 0.8",0.8450,0.3871,0.463,"0,458"
V19,"Fine-tuned hyperparameters of the XGBoost model using a random grid search with 300 iterations:

max_depth: [3, 5, 7, 10]
learning_rate: [0.01, 0.05, 0.1, 0.2]
n_estimators: [100, 200, 300]
subsample: [0.6, 0.8, 1.0]
colsample_bytree: [0.6, 0.8, 1.0]
reg_lambda: [0.1, 1, 10]
reg_alpha: [0, 1, 5]

Best Parameters:

subsample: 0.6
reg_lambda: 1
reg_alpha: 5
n_estimators: 200
max_depth: 3
learning_rate: 0.05
colsample_bytree: 1.0",0.8304,0.3974,0.461,0.457
V20-21,"Fine-tuned hyperparameters of the CatBoost model using a random grid search with 100 iterations:

depth: [3, 5, 7, 10]
learning_rate: [0.01, 0.05, 0.1, 0.2]
iterations: [100, 200, 300]
subsample: [0.6, 0.8, 1.0]
l2_leaf_reg: [0.1, 1, 10]
random_strength: [0, 1, 5]

Best Parameters:

subsample: 0.6
random_strength: 0
learning_rate: 0.05
l2_leaf_reg: 1
iterations: 100
depth: 5
",0.7088,0.3836,0.473,0.464
V22-23,"Fine-tuned hyperparameters of the LightGBM model using a random grid search with 100 iterations:

learning_rate: [0.01, 0.05, 0.1, 0.2]
max_depth: [8, 10, 12, 15]
num_leaves: [50, 100, 200, 500]
min_data_in_leaf: [5, 10, 15, 20]
feature_fraction: [0.7, 0.8, 0.9, 1.0]
bagging_fraction: [0.6, 0.7, 0.8, 1.0]
bagging_freq: [2, 4, 6]
lambda_l1: [0.1, 1, 5, 10]
lambda_l2: [0.01, 0.1, 1]

Best Parameters:

num_leaves: 100
min_data_in_leaf: 5
max_depth: 8
learning_rate: 0.05
lambda_l2: 0.1
lambda_l1: 10
feature_fraction: 0.8
bagging_freq: 6
bagging_fraction: 0.7",0.5848,0.3959,0.469,0.464
V24,"Increased model capacity due to underfitting
Increased max_depth in XGBoost to 10",0.7276,0.4002,0.470,0.461
V25,"Tried feature engineering, but performance was not very good, so reverted to v24",0.7469,0.4916,0.533,"0,374"
V26-27,Dropped season columns from the dataset,0.7202,0.4008,0.472,0.447
V28,Used three different models to train and combine their predictions for the final prediction using majority voting,,,,0.444
V29,Applied imputer on missing data,0.7031,0.3896,0.478,0.453
V30,Reverted and re-included the season columns,0.7142,0.3957,0.477,0.466
V31,Tried stacking models,0.6818,0.3996,0.471,0.456
V32,Reverted to v30 and applied median imputation to handle all missing data in both the train and test datasets,0.7142,0.3957,0.474,0.475
V33,Tried using mean imputation,0.7105,0.3938,0.480,0.455
V34,Tried using most frequent imputation,0.7130,0.3931,0.470,0.45
V35-36,Updated the Voting Regressor with scaling,0.7144,0.3949,0.477,0.468
V37,Added Random Forest to the voting ensemble,0.7775,0.3904,0.479,0.457
V38,Added weights to the models in the voting ensemble,0.6965,0.3934,0.478,0.461
V39,Increased the complexity of the model,0.7301,0.3881,0.477,0.465
V40,Fine-tuned the model further,0.7000,0.3937,0.471,0.459