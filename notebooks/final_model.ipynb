{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_style('darkgrid')\n",
    "from sklearn.metrics import make_scorer, cohen_kappa_score\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "from IPython.display import clear_output\n",
    "from scipy.optimize import minimize\n",
    "from colorama import Fore, Style\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads a parquet file, drops the 'step' column, and returns stats and ID\n",
    "def process_file(filename, dirname):\n",
    "    df = pd.read_parquet(os.path.join(dirname, filename, 'part-0.parquet'))\n",
    "    df.drop('step', axis=1, inplace=True)\n",
    "    return df.describe().values.reshape(-1), filename.split('=')[1]\n",
    "\n",
    "# Loads time-series data, processes each file, and compiles into a DataFrame\n",
    "def load_time_series(dirname) -> pd.DataFrame:\n",
    "    ids = os.listdir(dirname)\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(tqdm(executor.map(lambda fname: process_file(fname, dirname), ids), total=len(ids)))\n",
    "\n",
    "    stats, indexes = zip(*results)\n",
    "    df = pd.DataFrame(stats, columns=[f'stat_{i}' for i in range(len(stats[0]))])\n",
    "    df['id'] = indexes\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load static data\n",
    "train = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\n",
    "sample = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n",
    "\n",
    "# Load time series data for train and test\n",
    "train_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\")\n",
    "test_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract time-series feature columns by removing 'id' from train_ts columns\n",
    "time_series_cols = train_ts.columns.tolist()\n",
    "time_series_cols.remove(\"id\")\n",
    "time_series_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge time-series data with train and test datasets on 'id'\n",
    "train = pd.merge(train, train_ts, how=\"left\", on='id')\n",
    "test = pd.merge(test, test_ts, how=\"left\", on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'id' column from both train and test datasets\n",
    "train = train.drop('id', axis=1)\n",
    "test = test.drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of 58 features (excluding 'id' and including 'sii') that appear in both train and test datasets\n",
    "featuresCols = ['Basic_Demos-Enroll_Season', 'Basic_Demos-Age', 'Basic_Demos-Sex', \n",
    "                'CGAS-Season', 'CGAS-CGAS_Score', \n",
    "                'Physical-Season', 'Physical-BMI', 'Physical-Height', \n",
    "                'Physical-Weight', 'Physical-Waist_Circumference',\n",
    "                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n",
    "                'Fitness_Endurance-Season', 'Fitness_Endurance-Max_Stage',\n",
    "                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n",
    "                'FGC-Season', 'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n",
    "                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n",
    "                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n",
    "                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone', \n",
    "                'BIA-Season', 'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI', \n",
    "                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM', \n",
    "                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num', \n",
    "                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM', 'BIA-BIA_TBW', \n",
    "                'PAQ_A-Season', 'PAQ_A-PAQ_A_Total', 'PAQ_C-Season', 'PAQ_C-PAQ_C_Total', \n",
    "                'SDS-Season', 'SDS-SDS_Total_Raw', 'SDS-SDS_Total_T', \n",
    "                'PreInt_EduHx-Season', 'PreInt_EduHx-computerinternet_hoursday', \n",
    "                'sii']\n",
    "len(featuresCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the time series feature columns to the list of features\n",
    "featuresCols += time_series_cols\n",
    "\n",
    "# Select the specified features from the train dataset\n",
    "train = train[featuresCols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where the 'sii' column has missing values\n",
    "train = train.dropna(subset='sii')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the above steps, the train dataset contains 2,736 rows (down from 3,960) because rows with missing values in the 'sii' column were removed. It now has 155 columns: 58 features from the test set, 1 'sii' column (the target we want to predict), and 96 statistical features (from 0 to 95, derived from the time series data). The test dataset has 20 rows and 154 columns, retaining all the feature columns from the train dataset except for 'sii' (since this is the target to predict)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of categorical features that represent season data\n",
    "season_cat = ['Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season', \n",
    "              'Fitness_Endurance-Season', 'FGC-Season', 'BIA-Season', \n",
    "              'PAQ_A-Season', 'PAQ_C-Season', 'SDS-Season', 'PreInt_EduHx-Season']\n",
    "\n",
    "len(season_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to replace NaN with 'Missing' and change the type of season columns to 'category'\n",
    "def update(df):\n",
    "    global season_cat\n",
    "    for cat in season_cat:\n",
    "        df[cat] = df[cat].fillna('Missing')  # Replace NaN values with 'Missing'\n",
    "        df[cat] = df[cat].astype('category')  # Convert column to 'category' type to optimize storage\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the update function to the train and test datasets to handle missing values and optimize column types\n",
    "train = update(train)\n",
    "test = update(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create a mapping of category values to numbers\n",
    "def create_mapping(column, dataset):\n",
    "    unique_values = dataset[column].unique()  # Get unique category values from the column\n",
    "    return {value: idx for idx, value in enumerate(unique_values)}  # Create a mapping of category to number\n",
    "\n",
    "# Label encoding for season columns\n",
    "for cat in season_cat: \n",
    "    # Create mapping for train and test datasets\n",
    "    mapping_train = create_mapping(cat, train)\n",
    "    mapping_test = create_mapping(cat, test)\n",
    "\n",
    "    # Replace the categorical values with their corresponding numeric encoding and convert to integer data type\n",
    "    train[cat] = train[cat].replace(mapping_train).astype(int)\n",
    "    test[cat] = test[cat].replace(mapping_test).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate the quadratic weighted kappa score\n",
    "def quadratic_weighted_kappa(y_true, y_pred):\n",
    "    return cohen_kappa_score(y_true, y_pred, weights='quadratic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to round values based on thresholds\n",
    "def threshold_rounder(oof_non_rounded, thresholds):\n",
    "    # Apply thresholds to classify values into categories: 0, 1, 2, or 3\n",
    "    return np.where(oof_non_rounded < thresholds[0], 0,\n",
    "                   np.where(oof_non_rounded < thresholds[1], 1,\n",
    "                           np.where(oof_non_rounded < thresholds[2], 2, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to train the model, perform cross-validation, and evaluate using QWK\n",
    "def TrainML(model_class, test_data):\n",
    "    # Split data into features (X) and target (y)\n",
    "    X = train.drop(['sii'], axis=1)\n",
    "    y = train['sii']\n",
    "\n",
    "    # Define StratifiedKFold for cross-validation\n",
    "    SFK = StratifiedKFold(n_splits=5, shuffle=True, random_state=12)\n",
    "\n",
    "    train_S = []  # Store QWK scores for training\n",
    "    test_S = []   # Store QWK scores for validation\n",
    "\n",
    "    oof_non_rounded = np.zeros(len(y), dtype=float)  # Out-of-fold predictions (non-rounded)\n",
    "    oof_rounded = np.zeros(len(y), dtype=int)        # Out-of-fold predictions (rounded)\n",
    "    test_preds = np.zeros((len(test_data), 5))        # Store test predictions for each fold\n",
    "\n",
    "    # Loop over each fold for cross-validation\n",
    "    for fold, (train_idx, test_idx) in enumerate(tqdm(SFK.split(X, y), desc=\"Training Folds\", total=5)):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        # Clone and train the model\n",
    "        model = clone(model_class)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions on training and validation data\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_val_pred = model.predict(X_val)\n",
    "\n",
    "        # Store out-of-fold predictions\n",
    "        oof_non_rounded[test_idx] = y_val_pred\n",
    "        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n",
    "        oof_rounded[test_idx] = y_val_pred_rounded\n",
    "\n",
    "        # Calculate QWK for training and validation\n",
    "        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n",
    "        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n",
    "\n",
    "        train_S.append(train_kappa)\n",
    "        test_S.append(val_kappa)\n",
    "\n",
    "        # Store test predictions\n",
    "        test_preds[:, fold] = model.predict(test_data)\n",
    "\n",
    "        # Print fold results\n",
    "        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    # Print average QWK scores\n",
    "    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n",
    "    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n",
    "\n",
    "    # Optimize the threshold for better QWK performance\n",
    "    KappaOPtimizer = minimize(evaluate_predictions,\n",
    "                              x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded), \n",
    "                              method='Nelder-Mead')\n",
    "    \n",
    "    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n",
    "    \n",
    "    # Apply optimized thresholds to tune predictions\n",
    "    oof_tuned = threshold_rounder(oof_non_rounded, KappaOPtimizer.x)\n",
    "    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n",
    "\n",
    "    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n",
    "\n",
    "    # Average test predictions and apply tuned threshold\n",
    "    tpm = test_preds.mean(axis=1)\n",
    "    tpTuned = threshold_rounder(tpm, KappaOPtimizer.x)\n",
    "    \n",
    "    # Create submission DataFrame\n",
    "    submission = pd.DataFrame({\n",
    "        'id': sample['id'],\n",
    "        'sii': tpTuned\n",
    "    })\n",
    "\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for XGBoost model\n",
    "XGB_Params2 = {\n",
    "    'max_depth': 10,\n",
    "    'learning_rate': 0.05,\n",
    "    'n_estimators': 200,\n",
    "    'subsample': 0.6,\n",
    "    'colsample_bytree': 1.0,\n",
    "    'reg_lambda': 1,\n",
    "    'reg_alpha': 5,\n",
    "}\n",
    "\n",
    "# Parameters for CatBoost model\n",
    "CatBoost_Params1 = {\n",
    "    'depth': 5,\n",
    "    'learning_rate': 0.05,\n",
    "    'iterations': 100,\n",
    "    'subsample': 0.6,\n",
    "    'l2_leaf_reg': 1,\n",
    "    'random_strength': 0,\n",
    "}\n",
    "\n",
    "# Parameters for LightGBM model\n",
    "LGBM_Params1 = {\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 8,\n",
    "    'num_leaves': 100,\n",
    "    'min_data_in_leaf': 5,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.7,\n",
    "    'bagging_freq': 6,\n",
    "    'lambda_l1': 10,\n",
    "    'lambda_l2': 0.1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SimpleImputer with median strategy for imputing missing values\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "# Initialize individual models with optimized parameters\n",
    "optimized_XGB_model = XGBRegressor(**XGB_Params2, random_state=12)\n",
    "optimized_CatBoost_model = CatBoostRegressor(**CatBoost_Params1, random_state=12, silent=True)\n",
    "optimized_LGBM_model = LGBMRegressor(**LGBM_Params1, random_state=12)\n",
    "\n",
    "# Create a VotingRegressor ensemble with pipelines for each model\n",
    "voting_model = VotingRegressor(estimators=[\n",
    "    ('xgb', Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),  \n",
    "        ('scaler', StandardScaler()), \n",
    "        ('regressor', XGBRegressor(**XGB_Params2, random_state=12))  \n",
    "    ])),\n",
    "    ('cat', Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')), \n",
    "        ('regressor', CatBoostRegressor(**CatBoost_Params1, random_state=12, silent=True))  \n",
    "    ])),\n",
    "    ('lgb', Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')), \n",
    "        ('scaler', StandardScaler()), \n",
    "        ('regressor', LGBMRegressor(**LGBM_Params1, random_state=12)) \n",
    "    ])),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the ensemble VotingRegressor model and generate the submission\n",
    "submission = TrainML(voting_model, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
